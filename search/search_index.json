{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Libvirt Provider","text":"<p>The <code>libvirt-provider</code> project is a Libvirt based provider implementation of the ironcore types</p> <pre><code>graph TD\n    libvirt-provider -. implements .-&gt; compute.ironcore.dev</code></pre> <p>Namely <code>libvirt-provider</code> implements the <code>Machine</code> type. Additionally, it announces the available <code>MachineClasses</code> which are supported by the <code>MachinePool</code> based on configured criteria.</p> <p>Further information about the architecture and concepts of the <code>libvirt-provider</code> project can be found in the architecture section.</p>"},{"location":"architecture/","title":"Architecture","text":"<p>This section covers the core concepts of the <code>libvirt-provider</code> project.</p> <p>The <code>libvirt-provider</code> is an implementor of the <code>ironcore runtime interface</code> (<code>IRI</code>) for <code>Machines</code>, it implements the MachineRuntime</p> <p>A <code>libvirt-provider</code> is usually deployed along with a poollet. A poollet resolves dependencies, e.g. a secrets, and calls with the consolidated resource the <code>libvirt-provider</code>.  The <code>libvirt-provider</code> persists the required state and reconciles the resource in an asynchronous manner. </p> <p>The <code>libvirt-provider</code> interacts directly with a defined <code>libvirt</code> instance on the host.  A <code>Machine</code> is provisioned by creating a <code>domain xml</code>. If needed, images are downloaded, NICs are configured and volumes are attached.</p> <p>The following diagram visualizes the interplay of the different components:  <pre><code>graph TD\n    C([libvirt-provider])\n    P([machinepoollet])\n\n    P -- announces --&gt; VP[MachinePool]\n    P -- watches --&gt; V[Machines]\n\n    P -- uses IRI --&gt; C\n\n    C -- creates --&gt; I[Libvirt Domain XML]\n    C -- defines --&gt; VC[Supported MachineClasses]</code></pre></p>"},{"location":"usage/","title":"Libvirt-provider - usage documentation","text":""},{"location":"usage/#overview","title":"Overview","text":"<p><code>libvirt-provider</code> enables interaction with <code>libvirt</code> for managing virtual machine instances, integrating with multiple plugins for networking, storage, and more. It provides a flexible architecture to handle resources like VMs, volumes, and networking interfaces, with built-in support for garbage collection, volume size resyncing, and health monitoring. This guide provides a comprehensive usage flow, including configuration, flags, and practical examples, to get your <code>libvirt-provider</code> instance running.</p>"},{"location":"usage/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure you have the prerequisites described here.</p>"},{"location":"usage/#building-from-source","title":"Building from source","text":"<p>To build <code>libvirt-provider</code> from the source:</p> <ol> <li> <p>Clone the repository</p> <p>To bring up and start locally the libvirt-provider project, you first need to clone the repository.</p> <pre><code>git clone git@github.com:ironcore-dev/libvirt-provider.git\ncd libvirt-provider\n</code></pre> </li> <li> <p>Build the <code>libvirt-provider</code></p> <pre><code>make build\n</code></pre> </li> </ol>"},{"location":"usage/#configuration","title":"Configuration","text":"<p><code>libvirt-provider</code> is configured via command-line flags. To see the full list of configuration options, run:</p> <pre><code>libvirt-provider -h\n</code></pre>"},{"location":"usage/#required-flags","title":"Required flags","text":"<p>The following flags are required for the application to run properly:</p> <p><code>--supported-machine-classes</code> (Path to the supported machine classes file). Sample <code>machine-classes.json</code> can be found here.</p>"},{"location":"usage/#running-libvirt-provider","title":"Running libvirt-provider","text":"<p>Below is an example of configuring and running <code>libvirt-provider</code> with various flags:</p> <pre><code>go run cmd/libvirt-provider/main.go \\\n  --address /var/run/iri-machinebroker.sock \\\n  --streaming-address \":20251\" \\\n  --base-url \"http://localhost:20251\" \\\n  --libvirt-socket /var/run/libvirt/libvirt-sock \\\n  --libvirt-address \"unix:///var/run/libvirt/libvirt-sock\" \\\n  --libvirt-uri \"qemu:///system\" \\\n  --enable-hugepages \\\n  --qcow2-type \"qcow2\" \\\n  --volume-cache-policy \"writeback\" \\\n  --servers-metrics-address \":9090\" \\\n  --servers-health-check-address \":8080\" \\\n  --gc-vm-graceful-shutdown-timeout 5m \\\n  --gc-resync-interval 1m \\\n  --event-list-watch-source-resync-duration 1m \\\n  --supported-machine-classes \"/home/libvirt-provider/machine-classes.json\"\n</code></pre> <p>Once the libvirt-provider is started, which will handle various tasks like connecting to libvirt, managing VMs, and exposing various HTTP and gRPC endpoints for metrics, health checks, and more.</p>"},{"location":"usage/#server-endpoints","title":"Server endpoints","text":"<ol> <li> <p>gRPC endpoint: Exposes various gRPC services to manage virtual machines. Default address: <code>unix:///var/run/iri-machinebroker.sock</code></p> </li> <li> <p>Metrics server: Provides Prometheus-compatible metrics for monitoring. Default address: <code>\"\"</code> (if configured)</p> </li> <li> <p>Health check server: Provides a simple liveness check endpoint. Default address: <code>:8181</code> (if configured)</p> </li> <li> <p>Streaming server: Streams VM status and events. Default address: <code>:20251</code> (if configured)</p> </li> </ol> <p>You can access these services by connecting to their respective addresses.</p>"},{"location":"usage/#logs-and-debugging","title":"Logs and debugging","text":"<p>The service logs are critical for troubleshooting issues. By default, the service uses the <code>zap</code> logging framework, which supports structured logging and multiple log levels.</p> <p>To change the logging level:</p> <pre><code>go run cmd/libvirt-provider/main.go --zap-log-level debug\n</code></pre> <p>For more advanced troubleshooting, you can enable additional logging at different points in the execution flow.</p>"},{"location":"usage/#troubleshooting","title":"Troubleshooting","text":"<p>Here are some common issues you might encounter:</p>"},{"location":"usage/#libvirt-connection-errors","title":"Libvirt connection errors","text":"<p>If the <code>libvirt</code> socket or URI is incorrectly configured, the service will fail to connect to the hypervisor. Ensure that:</p> <ol> <li> <p>The <code>libvirt</code> socket path is correct.</p> </li> <li> <p>The <code>libvirt</code> URI is accessible (e.g., <code>qemu:///system</code> or <code>unix:///var/run/libvirt/libvirt-sock</code>).</p> </li> </ol>"},{"location":"usage/#vm-launch-failures","title":"VM launch failures","text":"<p>If VMs fail to launch, check the logs for specific errors related to machine classes or network plugins. You may need to adjust the supported machine classes or verify your network plugin configuration.</p>"},{"location":"concepts/console/","title":"Console","text":""},{"location":"concepts/events/","title":"Machine Events","text":""},{"location":"concepts/plugins/nic/","title":"Networkinterface Plugins","text":""},{"location":"concepts/plugins/volume/","title":"Volume Plugins","text":""},{"location":"development/dev_docs/","title":"libvirt-provider documentation","text":""},{"location":"development/dev_docs/#local-dev-setup","title":"Local dev setup","text":"<p>You can run the documentation via:</p> <pre><code>make start-docs\n</code></pre> <p>You can remove the <code>mkdocs</code> container image by running:</p> <pre><code>make clean-docs\n</code></pre>"},{"location":"development/dev_setup/","title":"Local Development Setup","text":"<ul> <li>Prerequisites</li> <li>Preperation</li> <li>Run libvirt-provider for local development</li> <li>Interact with the <code>libvirt-provider</code></li> <li>Deploy <code>libvirt-provider</code></li> </ul> <p>\u2139\ufe0f NOTE: To be able to take exec console of the machine, you can follow any one of the below approaches: - Run the <code>libvirt-provider</code> as the <code>libvirt-qemu</code> user. - Add user to <code>tty</code> group and create an entry with <code>devpts /dev/pts devpts rw,nosuid,noexec,relatime,gid=5,mode=0660 0 0</code> in <code>/etc/fstab</code>. - Manually ensure that you have <code>0660</code> access permissions on the character files created in <code>/dev/pts</code>.</p>"},{"location":"development/dev_setup/#prerequisites","title":"Prerequisites","text":"<ul> <li>Linux (code contains OS specific code)</li> <li>go &gt;= 1.20</li> <li><code>git</code>, <code>make</code> and <code>kubectl</code></li> <li>Access to a Kubernetes cluster (Minikube, kind or a   real cluster)</li> <li>libvirt</li> <li>QEMU</li> <li><code>irictl-machine</code> should be running locally or as container</li> </ul>"},{"location":"development/dev_setup/#preperation","title":"Preperation","text":""},{"location":"development/dev_setup/#setup-irictl-machine","title":"Setup <code>irictl-machine</code>","text":"<ol> <li> <p>Clone ironcore repository</p> <pre><code>git clone git@github.com:ironcore-dev/ironcore.git\ncd ironcore\n</code></pre> </li> <li> <p>Build <code>irictl-machine</code></p> <pre><code>go build -o bin/irictl-machine ./irictl-machine/cmd/irictl-machine/main.go\n</code></pre> </li> </ol>"},{"location":"development/dev_setup/#run-libvirt-provider-for-local-development","title":"Run libvirt-provider for local development","text":"<ol> <li> <p>Clone the Repository</p> <p>To bring up and start locally the libvirt-provider project for development purposes you first need to clone the repository.</p> <pre><code>git clone git@github.com:ironcore-dev/libvirt-provider.git\ncd libvirt-provider\n</code></pre> </li> <li> <p>Build the <code>libvirt-provider</code></p> <pre><code>make build\n</code></pre> </li> <li> <p>Run the <code>libvirt-provider</code></p> <p>The required libvirt-provider flags needs to be defined:</p> <pre><code>go run provider/cmd/main.go \\\n  --libvirt-provider-dir=&lt;path-to-initialize-libvirt-provider&gt; \\\n  --supported-machine-classes=&lt;path-to-machine-class-json&gt;/machine-classes.json \\\n  --network-interface-plugin-name=isolated \\\n  --address=&lt;local-path&gt;/iri-machinebroker.sock\n</code></pre> <p>Sample <code>machine-classes.json</code> can be found here.</p> </li> </ol>"},{"location":"development/dev_setup/#interact-with-the-libvirt-provider","title":"Interact with the <code>libvirt-provider</code>","text":"<ol> <li> <p>Creating machine</p> <pre><code>irictl-machine --address=unix:&lt;local-path-to-socket&gt;/iri-machinebroker.sock create machine -f &lt;path-to-machine-yaml&gt;/iri-machine.yaml\n</code></pre> <p>Sample <code>iri-machine.yaml</code>:</p> <pre><code>metadata:\n  id: 91076287116041d00fd421f43c3760389041dac4a8bd9201afba9a5baeb21c7\n  labels:\n    downward-api.machinepoollet.api.onmetal.de/root-machine-name: machine-hd4\n    downward-api.machinepoollet.api.onmetal.de/root-machine-namespace: default\n    downward-api.machinepoollet.api.onmetal.de/root-machine-uid: cab82eac-09d8-4428-9e6c-c98b40027b74\n    machinepoollet.api.onmetal.de/machine-name: machine-hd4\n    machinepoollet.api.onmetal.de/machine-namespace: default\n    machinepoollet.api.onmetal.de/machine-uid: cab82eac-09d8-4428-9e6c-c98b40027b74\nspec:\n  class: x3-small\n  image:\n    image: ghcr.io/ironcore-dev/ironcore-image/gardenlinux:rootfs-dev-20231206-v1\n  volumes:\n  - empty_disk:\n      size_bytes: 5368709120\n    name: ephe-disk\n    device: oda\n</code></pre> </li> <li> <p>Listing machines</p> <pre><code>irictl-machine --address=unix:&lt;local-path-to-socket&gt;/iri-machinebroker.sock get machine\n</code></pre> </li> <li> <p>Deleting machine</p> <pre><code>irictl-machine --address=unix:&lt;local-path-to-socket&gt;/iri-machinebroker.sock delete machine &lt;machine UUID&gt;\n</code></pre> </li> <li> <p>Taking machine console</p> <pre><code>irictl-machine --address=unix:&lt;local-path-to-socket&gt;/iri-machinebroker.sock exec &lt;machine UUID&gt;\n</code></pre> </li> </ol>"},{"location":"development/dev_setup/#deploy-libvirt-provider","title":"Deploy <code>libvirt-provider</code>","text":"<p>\u2139\ufe0f NOTE: If the <code>libvirt-uri</code> can not be auto-detected it can be defined via flag: e.g. <code>--libvirt-uri=qemu:///session</code> \u2139\ufe0f NOTE: For trying out the controller use the <code>isolated</code> network interface plugin: <code>--network-interface-plugin-name=isolated</code> \u2139\ufe0f NOTE: Libvirt-provider can run directly as binary program on worker node</p> <ol> <li> <p>Make docker images</p> <pre><code>make docker-build\n</code></pre> </li> <li> <p>Deploy virtlet as kubernetes</p> <pre><code>make deploy\n</code></pre> </li> </ol>"}]}